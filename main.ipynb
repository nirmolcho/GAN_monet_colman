{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T08:35:42.091326Z",
     "start_time": "2024-02-09T08:35:41.795645Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m layers, models , initializers\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minitializers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RandomNormal\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models , initializers\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_dir(directory, target_size=(320, 320)):\n",
    "    image_list = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            # Load the image using OpenCV\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Warning: Unable to load image '{filename}'\")\n",
    "                continue\n",
    "            \n",
    "            # Convert BGR to RGB format\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Resize the image\n",
    "            resized_image = cv2.resize(image_rgb, target_size)\n",
    "            image_list.append(resized_image)\n",
    "    return image_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"data/monet_jpg/\"\n",
    "monet_image_list = load_images_from_dir(directory_path)\n",
    "print(\"Number of images loaded:\", len(monet_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"data/photo_jpg/\"\n",
    "image_list = load_images_from_dir(directory_path)\n",
    "print(\"Number of images loaded:\", len(image_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_images(image_list):\n",
    "    # Ensure that we have at least 5 images\n",
    "    if len(image_list) < 5:\n",
    "        print(\"Error: Insufficient number of images.\")\n",
    "        return\n",
    "    \n",
    "    # Choose 5 random indices\n",
    "    random_indices = random.sample(range(len(image_list)), 5)\n",
    "    \n",
    "    # Plot the images\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        axes[i].imshow(image_list[idx])  # Convert BGR to RGB for Matplotlib\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"Image {idx}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_images(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_images(monet_image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_white_corner(image):\n",
    "    # Convert image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Define the size of the corner region\n",
    "    corner_size = 5\n",
    "    \n",
    "    # Define the four corners\n",
    "    corners = [\n",
    "        gray_image[:corner_size, :corner_size],                # Top-left corner\n",
    "        gray_image[:corner_size, -corner_size:],              # Top-right corner\n",
    "        gray_image[-corner_size:, :corner_size],              # Bottom-left corner\n",
    "        gray_image[-corner_size:, -corner_size:]              # Bottom-right corner\n",
    "    ]\n",
    "    \n",
    "    # Check if all corners are completely white\n",
    "    return all(np.all(corner > 220) for corner in corners)\n",
    "\n",
    "def crop_and_resize(image, target_size=(320, 320)):\n",
    "    # Get the dimensions of the input image\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Calculate the center coordinates\n",
    "    center_x = width // 2\n",
    "    center_y = height // 2\n",
    "    \n",
    "    # Calculate the crop boundaries\n",
    "    crop_left = max(0, center_x - 215 // 2)\n",
    "    crop_top = max(0, center_y - 215 // 2)\n",
    "    crop_right = min(width, center_x + 215 // 2)\n",
    "    crop_bottom = min(height, center_y + 215 // 2)\n",
    "    \n",
    "    # Crop the image\n",
    "    cropped_image = image[crop_top:crop_bottom, crop_left:crop_right]\n",
    "    \n",
    "    # Resize the cropped image\n",
    "    resized_image = cv2.resize(cropped_image, target_size)\n",
    "    \n",
    "    return resized_image\n",
    "\n",
    "def show_images_with_white_corner(images):\n",
    "    for idx, image in enumerate(images):\n",
    "        if has_white_corner(image):\n",
    "            cropped_resized_image = crop_and_resize(image)\n",
    "            plt.imshow(image)\n",
    "            plt.title(f\"Image {idx}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            plt.imshow(cropped_resized_image)\n",
    "            plt.title(f\"Image {idx}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "show_images_with_white_corner(monet_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_images_with_white_corner(images):\n",
    "    new_images = []\n",
    "    for idx, image in enumerate(images):\n",
    "        if has_white_corner(image):\n",
    "            new_images.append(crop_and_resize(image))\n",
    "        else:\n",
    "            new_images.append(image)\n",
    "\n",
    "    return new_images\n",
    "\n",
    "monet_image_list = change_images_with_white_corner(monet_image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_resize_quarters(original_image):\n",
    "    # Convert PIL Image to NumPy array\n",
    "    original_image_array = np.array(original_image)\n",
    "    \n",
    "    # Ensure the image is 320x320\n",
    "    if original_image_array.shape[:2] != (320, 320):\n",
    "        print(\"Error: Image size must be 320x320.\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    # Split the image into quarters\n",
    "    quarters = []\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            left = j * 160\n",
    "            upper = i * 160\n",
    "            right = left + 160\n",
    "            lower = upper + 160\n",
    "            quarter = original_image_array[upper:lower, left:right, :]\n",
    "            quarters.append(quarter)\n",
    "    \n",
    "    # Resize each quarter to 320x320\n",
    "    resized_quarters = [cv2.resize(quarter, (320, 320)) for quarter in quarters]\n",
    "    \n",
    "    return original_image , resized_quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image, quarters = split_and_resize_quarters(monet_image_list[0])\n",
    "if quarters:\n",
    "    # Plot original image\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(original_image)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot each quarter\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    for i, quarter in enumerate(quarters):\n",
    "        axes[i//2, i%2].imshow(quarter)\n",
    "        axes[i//2, i%2].axis('off')\n",
    "        axes[i//2, i%2].set_title(f\"Quarter {i+1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_with_high_color_variance(images, threshold):\n",
    "    high_color_var_images = []\n",
    "    \n",
    "    for image in images:\n",
    "        # Convert image to numpy array\n",
    "        image_array = np.array(image)\n",
    "        \n",
    "        # Calculate color variance\n",
    "        color_variance = np.var(image_array)\n",
    "        # Check if color variance is higher than the threshold\n",
    "        if color_variance > threshold:\n",
    "            high_color_var_images.append(image)\n",
    "    \n",
    "    return high_color_var_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_colorful_image = get_images_with_high_color_variance(quarters , 1000)\n",
    "if most_colorful_image:\n",
    "    # Plot each quarter\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    for i, quarter in enumerate(most_colorful_image):\n",
    "        axes[i//2, i%2].imshow(quarter)\n",
    "        axes[i//2, i%2].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirror_flip(image):\n",
    "    # Mirror flip the image horizontally\n",
    "    flipped_image = cv2.flip(image, 1)\n",
    "    return flipped_image\n",
    "\n",
    "\n",
    "def aug_pipeline(images, threshold):\n",
    "    augmented_images = []\n",
    "    \n",
    "    for image in images:\n",
    "        # Split into quarters\n",
    "        _ , quarters = split_and_resize_quarters(image)\n",
    "        \n",
    "        # Save only important quarters\n",
    "        important_quarters = get_images_with_high_color_variance(quarters, threshold)\n",
    "\n",
    "        # Mirror flip the original image\n",
    "        mirrored_image = mirror_flip(image)\n",
    "                \n",
    "        # Combine original image, mirrored image, and important quarters\n",
    "        augmented_images.append(image)\n",
    "        augmented_images.append(mirrored_image)\n",
    "        if len(important_quarters) != 0:\n",
    "            augmented_images.extend(important_quarters)\n",
    "    \n",
    "    return augmented_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(monet_image_list))\n",
    "augmented_images = aug_pipeline(monet_image_list, 1000)\n",
    "print(len(augmented_images))\n",
    "plot_random_images(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the data in batch for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 10\n",
    "\n",
    "# Convert lists to TensorFlow datasets\n",
    "monet_dataset = tf.data.Dataset.from_tensor_slices(augmented_images)\n",
    "photo_dataset = tf.data.Dataset.from_tensor_slices(image_list)\n",
    "\n",
    "# Zip the datasets to combine them\n",
    "combined_dataset = tf.data.Dataset.zip((monet_dataset, photo_dataset))\n",
    "\n",
    "# Optionally shuffle and batch the dataset\n",
    "combined_dataset = combined_dataset.shuffle(buffer_size=len(augmented_images)).batch(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images_from_batch(dataset, title):\n",
    "    # Get one batch of images from the dataset\n",
    "    images_batch = next(iter(dataset.batch(10)))  # Assuming batch size of 16\n",
    "    # Create a figure with a larger size\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Plot each image in the batch\n",
    "    for i, image in enumerate(images_batch):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(image.numpy().astype(\"uint8\"))  # Convert to uint8 for plotting\n",
    "        plt.axis(\"off\")\n",
    "    # Set title and show plot\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot one batch of Monet images\n",
    "plot_images_from_batch(monet_dataset, \"Monet Images\")\n",
    "\n",
    "# Plot one batch of photo images\n",
    "plot_images_from_batch(photo_dataset, \"Photo Images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create the objects that neccessary for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "  if apply_batchnorm:\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  return result\n",
    "\n",
    "\n",
    "sample = next(iter(monet_dataset.batch(1)))  # Assuming batch size of 1\n",
    "sample_float32 = tf.cast(sample, tf.float32)\n",
    "\n",
    "down_model = downsample(3, 4)\n",
    "down_result = down_model(sample_float32)\n",
    "print(down_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "  result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "      result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result\n",
    "     \n",
    "\n",
    "\n",
    "up_model = upsample(3, 4)\n",
    "up_result = up_model(down_result)\n",
    "print (up_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define Generator model\n",
    "def Generator():\n",
    "  inputs = tf.keras.layers.Input(shape=[320, 320, 3])\n",
    "  \n",
    "  down_stack = [\n",
    "    downsample(80, 4, apply_batchnorm=False),  # (batch_size, 160, 160, 80)\n",
    "    downsample(160, 4),  # (batch_size, 64, 64, 128)\n",
    "    downsample(320, 4),  # (batch_size, 32, 32, 256)\n",
    "    downsample(640, 4),  # (batch_size, 16, 16, 512)\n",
    "    downsample(640, 4),  # (batch_size, 8, 8, 512)\n",
    "    downsample(640, 4),  # (batch_size, 4, 4, 512)\n",
    "    # downsample(640, 4),  # (batch_size, 2, 2, 512)\n",
    "    # downsample(640, 4),  # (batch_size, 1, 1, 512)\n",
    "  ]\n",
    "\n",
    "  up_stack = [\n",
    "    # upsample(640, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
    "    # upsample(640, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "    upsample(640, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "    upsample(640, 4 ,apply_dropout=True),  # (batch_size, 16, 16, 1024)\n",
    "    upsample(320, 4),  # (batch_size, 32, 32, 512)\n",
    "    upsample(160, 4),  # (batch_size, 64, 64, 256)\n",
    "    upsample(80, 4),  # (batch_size, 128, 128, 128)\n",
    "  ]\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(3, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
    "\n",
    "\n",
    "\n",
    "  x = inputs\n",
    "\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "  x = last(x)\n",
    "     \n",
    "\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "# Test the generator model with random input batch\n",
    "def test_build_generator():\n",
    "\n",
    "\n",
    "    # Build generator model\n",
    "    generator = Generator()\n",
    "    \n",
    "\n",
    "    input_shape = (320, 320, 3)  # Example input shape\n",
    "    # Generate a random batch of input images\n",
    "    batch_size = 4\n",
    "    random_input_batch = tf.random.uniform((batch_size,) + input_shape, minval=0, maxval=1)\n",
    "    \n",
    "    # Forward pass through the generator\n",
    "    generated_images = generator(random_input_batch)\n",
    "    print(generator.summary())\n",
    "\n",
    "    # Check output shape\n",
    "    output_shape = generated_images.shape\n",
    "    expected_output_shape = (batch_size, 320, 320, 3)  # Output shape with batch dimension\n",
    "    \n",
    "    # Check if the output shape matches the expected output shape\n",
    "    assert output_shape == expected_output_shape, \"Output shape mismatch\"\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_build_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Discriminator model\n",
    "def Discriminator(input_nc = 3, ndf=64, n_layers=3):\n",
    "    inputs = tf.keras.layers.Input(shape=(None, None, input_nc))\n",
    "    x = inputs\n",
    "    nf_mult_prev = 1\n",
    "\n",
    "    # First convolutional layer\n",
    "    x = tf.keras.layers.Conv2D(ndf, kernel_size=4, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    nf_mult = 1\n",
    "    # Middle convolutional layers\n",
    "    for n in range(1, n_layers):\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n, 8)\n",
    "        x = tf.keras.layers.Conv2D(ndf * nf_mult, kernel_size=4, strides=2, padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    # Last convolutional layer\n",
    "    nf_mult_prev = nf_mult\n",
    "    nf_mult = min(2 ** n_layers, 8)\n",
    "    x = tf.keras.layers.Conv2D(ndf * nf_mult, kernel_size=4, strides=1, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    # Output convolutional layer\n",
    "    outputs = tf.keras.layers.Conv2D(1, kernel_size=4, strides=1, padding='same')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Test the discriminator model with random input batch\n",
    "def test_build_discriminator():\n",
    "    # Define input shape\n",
    "    input_shape = (320, 320, 3)  # Example input shape\n",
    "\n",
    "    # Build discriminator model\n",
    "    discriminator = Discriminator(3)\n",
    "    \n",
    "    # Generate a random batch of input images\n",
    "    batch_size = 4\n",
    "    random_input_batch = tf.random.uniform((batch_size,) + input_shape, minval=0, maxval=1)\n",
    "    \n",
    "    # Forward pass through the discriminator\n",
    "    discriminator_output = discriminator(random_input_batch)\n",
    "    \n",
    "    # Check output shape\n",
    "    output_shape = discriminator_output.shape\n",
    "    print(output_shape)\n",
    "    print(discriminator.summary())\n",
    "    expected_output_shape = (batch_size, 40, 40, 1)  # Output shape with batch dimension\n",
    "    \n",
    "    # Check if the output shape matches the expected output shape\n",
    "    assert output_shape == expected_output_shape, \"Output shape mismatch\"\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_build_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial Loss\n",
    "def adversarial_loss(discriminator, generated):\n",
    "    fake_output = discriminator(generated)\n",
    "    return tf.reduce_mean(tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(fake_output), fake_output))\n",
    "\n",
    "\n",
    "# Cycle Consistency Loss\n",
    "def cycle_consistency_loss(real_images, cycled_images, lambda_weight=10):\n",
    "    real_images = tf.cast(real_images, tf.float32)\n",
    "    cycled_images = tf.cast(cycled_images, tf.float32)\n",
    "    loss = tf.reduce_mean(tf.abs(real_images - cycled_images))\n",
    "    return lambda_weight * loss\n",
    "\n",
    "# Identity Loss\n",
    "def identity_loss(real_images, same_images, lambda_weight=0.5):\n",
    "    real_images = tf.cast(real_images, tf.float32)\n",
    "    same_images = tf.cast(same_images, tf.float32)\n",
    "    loss = tf.reduce_mean(tf.abs(real_images - same_images))\n",
    "    return lambda_weight * 0.5 * loss\n",
    "\n",
    "\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_batch(generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet, batch_real, batch_monet):\n",
    "    with tf.GradientTape() as gen_R2M_tape, tf.GradientTape() as gen_M2R_tape, \\\n",
    "         tf.GradientTape() as disc_r_tape, tf.GradientTape() as disc_m_tape:\n",
    "\n",
    "        # Generate fake images\n",
    "        fake_monet = generator_real2monet(batch_real, training=True)\n",
    "        fake_real = generator_monet2real(batch_monet, training=True)\n",
    "\n",
    "        # Generate reconstructed images\n",
    "        reconstr_real = generator_monet2real(fake_monet, training=True)\n",
    "        reconstr_monet = generator_real2monet(fake_real, training=True)\n",
    "\n",
    "        # Identity mapping loss\n",
    "        identity_loss_real = identity_loss(batch_real, reconstr_real)\n",
    "        identity_loss_monet = identity_loss(batch_monet, reconstr_monet)\n",
    "\n",
    "        # Adversarial loss\n",
    "        adv_loss_R2M = adversarial_loss(discriminator_monet, fake_monet)\n",
    "        adv_loss_M2R = adversarial_loss(discriminator_real, fake_real)\n",
    "\n",
    "\n",
    "        # Cycle consistency loss\n",
    "        cycle_loss = cycle_consistency_loss(batch_real, reconstr_real) + \\\n",
    "                     cycle_consistency_loss(batch_monet, reconstr_monet)\n",
    "\n",
    "        # Total generator loss\n",
    "        total_gen_R2M_loss = adv_loss_R2M + cycle_loss\n",
    "        total_gen_M2R_loss = adv_loss_M2R + cycle_loss\n",
    "\n",
    "    # Compute gradients of generator loss with respect to generator variables\n",
    "    gen_R2M_gradients = gen_R2M_tape.gradient(total_gen_R2M_loss, generator_real2monet.trainable_variables)\n",
    "    gen_M2R_gradients = gen_M2R_tape.gradient(total_gen_M2R_loss, generator_monet2real.trainable_variables)\n",
    "\n",
    "    # Apply gradients to generator variables\n",
    "    generator_real2monet.optimizer.apply_gradients(zip(gen_R2M_gradients, generator_real2monet.trainable_variables))\n",
    "    generator_monet2real.optimizer.apply_gradients(zip(gen_M2R_gradients, generator_monet2real.trainable_variables))\n",
    "\n",
    "    # Train discriminator X\n",
    "    with tf.GradientTape() as disc_r_tape:\n",
    "        disc_real_real_output = discriminator_real(batch_real, training=True)\n",
    "        disc_real_fake_output = discriminator_real(fake_real, training=True)\n",
    "\n",
    "        # Compute discriminator X loss\n",
    "        disc_real_loss = discriminator_loss(disc_real_real_output, disc_real_fake_output)\n",
    "\n",
    "    # Compute gradients of discriminator X loss with respect to discriminator X variables\n",
    "    disc_real_gradients = disc_r_tape.gradient(disc_real_loss, discriminator_real.trainable_variables)\n",
    "\n",
    "    # Apply gradients to discriminator X variables\n",
    "    discriminator_real.optimizer.apply_gradients(zip(disc_real_gradients, discriminator_real.trainable_variables))\n",
    "\n",
    "    # Train discriminator Y\n",
    "    with tf.GradientTape() as disc_m_tape:\n",
    "        disc_monet_real_output = discriminator_monet(batch_monet, training=True)\n",
    "        disc_monet_fake_output = discriminator_monet(fake_monet, training=True)\n",
    "\n",
    "        # Compute discriminator Y loss\n",
    "        disc_monet_loss = discriminator_loss(disc_monet_real_output, disc_monet_fake_output)\n",
    "\n",
    "    # Compute gradients of discriminator Y loss with respect to discriminator Y variables\n",
    "    disc_monet_gradients = disc_m_tape.gradient(disc_monet_loss, discriminator_monet.trainable_variables)\n",
    "\n",
    "    # Apply gradients to discriminator Y variables\n",
    "    discriminator_monet.optimizer.apply_gradients(zip(disc_monet_gradients, discriminator_monet.trainable_variables))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet, combined_dataset, epochs):\n",
    "    num_batches = len(combined_dataset)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        # Use tqdm to create a progress bar for the batches\n",
    "        progress_bar = tqdm(combined_dataset, total=num_batches, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        for batch_real, batch_monet in progress_bar:\n",
    "            train_one_batch(generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet, batch_real, batch_monet)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build and compile the generators\n",
    "generator_real2monet = Generator()\n",
    "generator_monet2real = Generator()\n",
    "generator_real2monet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), loss=adversarial_loss)\n",
    "generator_monet2real.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), loss=adversarial_loss)\n",
    "\n",
    "# Build and compile the discriminators\n",
    "discriminator_real = Discriminator()\n",
    "discriminator_monet = Discriminator()\n",
    "discriminator_real.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), loss=adversarial_loss)\n",
    "discriminator_monet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), loss=adversarial_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet, combined_dataset, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cycle_gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

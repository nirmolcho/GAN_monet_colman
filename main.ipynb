{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models , initializers\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.optimizers import legacy\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_images_from_dir(directory, target_size=(320, 320)):\n",
    "    image_list = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            # Load the image using OpenCV\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Warning: Unable to load image '{filename}'\")\n",
    "                continue\n",
    "            \n",
    "            # Convert BGR to RGB format\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Resize the image\n",
    "            resized_image = cv2.resize(image_rgb, target_size)\n",
    "            image_list.append(resized_image)\n",
    "    return image_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "directory_path = \"data/monet_jpg/\"\n",
    "monet_image_list = load_images_from_dir(directory_path)\n",
    "print(\"Number of images loaded:\", len(monet_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "directory_path = \"data/photo_jpg/\"\n",
    "image_list = load_images_from_dir(directory_path)\n",
    "print(\"Number of images loaded:\", len(image_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# EDA images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_random_images(image_list):\n",
    "    # Ensure that we have at least 5 images\n",
    "    if len(image_list) < 5:\n",
    "        print(\"Error: Insufficient number of images.\")\n",
    "        return\n",
    "    \n",
    "    # Choose 5 random indices\n",
    "    random_indices = random.sample(range(len(image_list)), 5)\n",
    "    \n",
    "    # Plot the images\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        axes[i].imshow(image_list[idx])  # Convert BGR to RGB for Matplotlib\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"Image {idx}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_random_images(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_random_images(monet_image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def has_white_corner(image):\n",
    "    # Convert image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Define the size of the corner region\n",
    "    corner_size = 5\n",
    "    \n",
    "    # Define the four corners\n",
    "    corners = [\n",
    "        gray_image[:corner_size, :corner_size],                # Top-left corner\n",
    "        gray_image[:corner_size, -corner_size:],              # Top-right corner\n",
    "        gray_image[-corner_size:, :corner_size],              # Bottom-left corner\n",
    "        gray_image[-corner_size:, -corner_size:]              # Bottom-right corner\n",
    "    ]\n",
    "    \n",
    "    # Check if all corners are completely white\n",
    "    return all(np.all(corner > 220) for corner in corners)\n",
    "\n",
    "def crop_and_resize(image, target_size=(320, 320)):\n",
    "    # Get the dimensions of the input image\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Calculate the center coordinates\n",
    "    center_x = width // 2\n",
    "    center_y = height // 2\n",
    "    \n",
    "    # Calculate the crop boundaries\n",
    "    crop_left = max(0, center_x - 215 // 2)\n",
    "    crop_top = max(0, center_y - 215 // 2)\n",
    "    crop_right = min(width, center_x + 215 // 2)\n",
    "    crop_bottom = min(height, center_y + 215 // 2)\n",
    "    \n",
    "    # Crop the image\n",
    "    cropped_image = image[crop_top:crop_bottom, crop_left:crop_right]\n",
    "    \n",
    "    # Resize the cropped image\n",
    "    resized_image = cv2.resize(cropped_image, target_size)\n",
    "    \n",
    "    return resized_image\n",
    "\n",
    "def show_images_with_white_corner(images):\n",
    "    for idx, image in enumerate(images):\n",
    "        if has_white_corner(image):\n",
    "            cropped_resized_image = crop_and_resize(image)\n",
    "            plt.imshow(image)\n",
    "            plt.title(f\"Image {idx}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            plt.imshow(cropped_resized_image)\n",
    "            plt.title(f\"Image {idx}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "show_images_with_white_corner(monet_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def change_images_with_white_corner(images):\n",
    "    new_images = []\n",
    "    for idx, image in enumerate(images):\n",
    "        if has_white_corner(image):\n",
    "            new_images.append(crop_and_resize(image))\n",
    "        else:\n",
    "            new_images.append(image)\n",
    "\n",
    "    return new_images\n",
    "\n",
    "monet_image_list = change_images_with_white_corner(monet_image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_and_resize_quarters(original_image):\n",
    "    # Convert PIL Image to NumPy array\n",
    "    original_image_array = np.array(original_image)\n",
    "    \n",
    "    # Ensure the image is 320x320\n",
    "    if original_image_array.shape[:2] != (320, 320):\n",
    "        print(\"Error: Image size must be 320x320.\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    # Split the image into quarters\n",
    "    quarters = []\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            left = j * 160\n",
    "            upper = i * 160\n",
    "            right = left + 160\n",
    "            lower = upper + 160\n",
    "            quarter = original_image_array[upper:lower, left:right, :]\n",
    "            quarters.append(quarter)\n",
    "    \n",
    "    # Resize each quarter to 320x320\n",
    "    resized_quarters = [cv2.resize(quarter, (320, 320)) for quarter in quarters]\n",
    "    \n",
    "    return original_image , resized_quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_image, quarters = split_and_resize_quarters(monet_image_list[0])\n",
    "if quarters:\n",
    "    # Plot original image\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(original_image)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot each quarter\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    for i, quarter in enumerate(quarters):\n",
    "        axes[i//2, i%2].imshow(quarter)\n",
    "        axes[i//2, i%2].axis('off')\n",
    "        axes[i//2, i%2].set_title(f\"Quarter {i+1}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_images_with_high_color_variance(images, threshold):\n",
    "    high_color_var_images = []\n",
    "    \n",
    "    for image in images:\n",
    "        # Convert image to numpy array\n",
    "        image_array = np.array(image)\n",
    "        \n",
    "        # Calculate color variance\n",
    "        color_variance = np.var(image_array)\n",
    "        # Check if color variance is higher than the threshold\n",
    "        if color_variance > threshold:\n",
    "            high_color_var_images.append(image)\n",
    "    \n",
    "    return high_color_var_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "most_colorful_image = get_images_with_high_color_variance(quarters , 1000)\n",
    "if most_colorful_image:\n",
    "    # Plot each quarter\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    for i, quarter in enumerate(most_colorful_image):\n",
    "        axes[i//2, i%2].imshow(quarter)\n",
    "        axes[i//2, i%2].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mirror_flip(image):\n",
    "    # Mirror flip the image horizontally\n",
    "    flipped_image = cv2.flip(image, 1)\n",
    "    return flipped_image\n",
    "\n",
    "\n",
    "def aug_pipeline(images, threshold):\n",
    "    augmented_images = []\n",
    "    \n",
    "    for image in images:\n",
    "        # Split into quarters\n",
    "        _ , quarters = split_and_resize_quarters(image)\n",
    "        \n",
    "        # Save only important quarters\n",
    "        important_quarters = get_images_with_high_color_variance(quarters, threshold)\n",
    "\n",
    "        # Mirror flip the original image\n",
    "        mirrored_image = mirror_flip(image)\n",
    "                \n",
    "        # Combine original image, mirrored image, and important quarters\n",
    "        augmented_images.append(image)\n",
    "        augmented_images.append(mirrored_image)\n",
    "        if len(important_quarters) != 0:\n",
    "            augmented_images.extend(important_quarters)\n",
    "    \n",
    "    return augmented_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(monet_image_list))\n",
    "augmented_images = aug_pipeline(monet_image_list, 1000)\n",
    "print(len(augmented_images))\n",
    "plot_random_images(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# save the data in batch for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debug\n",
    "augmented_images = augmented_images[:100]\n",
    "image_list = image_list[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "BATCH_SIZE = 20 # need to select the batch size based on the gpu\n",
    "\n",
    "monet_len, real_len = len(augmented_images), len(image_list)\n",
    "\n",
    "\n",
    "\n",
    "# Correct the slicing for augmented_images to split into train, val, and test sets\n",
    "train_monet = augmented_images[:int(0.8 * monet_len)]\n",
    "val_monet = augmented_images[int(0.8 * monet_len):int(0.9 * monet_len)]\n",
    "test_monet = augmented_images[int(0.9 * monet_len):]\n",
    "\n",
    "# Correct the slicing for image_list to split into train, val, and test sets\n",
    "train_real = image_list[:int(0.8 * real_len)]\n",
    "val_real = image_list[int(0.8 * real_len):int(0.9 * real_len)]\n",
    "test_real = image_list[int(0.9 * real_len):]\n",
    "\n",
    "\n",
    "# Convert lists to TensorFlow datasets\n",
    "train_monet = tf.data.Dataset.from_tensor_slices(train_monet)\n",
    "val_monet = tf.data.Dataset.from_tensor_slices(val_monet)\n",
    "test_monet = tf.data.Dataset.from_tensor_slices(test_monet)\n",
    "\n",
    "train_real = tf.data.Dataset.from_tensor_slices(train_real)\n",
    "val_real = tf.data.Dataset.from_tensor_slices(val_real)\n",
    "test_real = tf.data.Dataset.from_tensor_slices(test_real)\n",
    "\n",
    "# Train Dataset\n",
    "train_dataset = tf.data.Dataset.zip((train_monet, train_real))\n",
    "test_dataset = tf.data.Dataset.zip((test_monet, test_real))\n",
    "val_dataset = tf.data.Dataset.zip((val_monet, val_real))\n",
    "\n",
    "\n",
    "# Optionally shuffle and batch the dataset\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(train_monet)).batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "def plot_images_from_batch(dataset, title, batch_size=10):\n",
    "    # Fetch one batch of images\n",
    "    for monet_batch, real_batch in dataset.take(1):  # Just take one batch from the dataset\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Plot Monet images\n",
    "            plt.subplot(2, batch_size, i + 1)\n",
    "            monet_image = monet_batch[i].numpy().astype(\"uint8\")\n",
    "            plt.imshow(monet_image)\n",
    "            plt.title(\"Monet\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Plot Real images, adjust subplot index to move to the next row\n",
    "            plt.subplot(2, batch_size, i + 1 + batch_size)\n",
    "            real_image = real_batch[i].numpy().astype(\"uint8\")\n",
    "            plt.imshow(real_image)\n",
    "            plt.title(\"Real\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_images_from_batch(train_dataset, \"Monet and Real Images\", batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# create the objects that neccessary for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "  if apply_batchnorm:\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  return result\n",
    "\n",
    "\n",
    "sample, _ = next(iter(train_dataset))  # Assuming batch size of 1\n",
    "sample_float32 = tf.cast(sample, tf.float32)\n",
    "\n",
    "down_model = downsample(3, 4)\n",
    "down_result = down_model(sample_float32)\n",
    "print(down_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "  result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "      result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result\n",
    "     \n",
    "\n",
    "\n",
    "up_model = upsample(3, 4)\n",
    "up_result = up_model(down_result)\n",
    "print (up_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define Generator model\n",
    "def Generator():\n",
    "  inputs = tf.keras.layers.Input(shape=[320, 320, 3])\n",
    "  \n",
    "  down_stack = [\n",
    "    downsample(80, 4, apply_batchnorm=False),  # (batch_size, 160, 160, 80)\n",
    "    downsample(160, 4),  # (batch_size, 64, 64, 128)\n",
    "    downsample(320, 4),  # (batch_size, 32, 32, 256)\n",
    "    downsample(640, 4),  # (batch_size, 16, 16, 512)\n",
    "    downsample(640, 4),  # (batch_size, 8, 8, 512)\n",
    "    downsample(640, 4),  # (batch_size, 4, 4, 512)\n",
    "    # downsample(640, 4),  # (batch_size, 2, 2, 512)\n",
    "    # downsample(640, 4),  # (batch_size, 1, 1, 512)\n",
    "  ]\n",
    "\n",
    "  up_stack = [\n",
    "    # upsample(640, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
    "    # upsample(640, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "    upsample(640, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "    upsample(640, 4 ,apply_dropout=True),  # (batch_size, 16, 16, 1024)\n",
    "    upsample(320, 4),  # (batch_size, 32, 32, 512)\n",
    "    upsample(160, 4),  # (batch_size, 64, 64, 256)\n",
    "    upsample(80, 4),  # (batch_size, 128, 128, 128)\n",
    "  ]\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(3, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
    "\n",
    "\n",
    "\n",
    "  x = inputs\n",
    "\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "  x = last(x)\n",
    "     \n",
    "\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "# Test the generator model with random input batch\n",
    "def test_build_generator():\n",
    "\n",
    "\n",
    "    # Build generator model\n",
    "    generator = Generator()\n",
    "    \n",
    "\n",
    "    input_shape = (320, 320, 3)  # Example input shape\n",
    "    # Generate a random batch of input images\n",
    "    batch_size = 4\n",
    "    random_input_batch = tf.random.uniform((batch_size,) + input_shape, minval=0, maxval=1)\n",
    "    \n",
    "    # Forward pass through the generator\n",
    "    generated_images = generator(random_input_batch)\n",
    "    print(generator.summary())\n",
    "\n",
    "    # Check output shape\n",
    "    output_shape = generated_images.shape\n",
    "    expected_output_shape = (batch_size, 320, 320, 3)  # Output shape with batch dimension\n",
    "    \n",
    "    # Check if the output shape matches the expected output shape\n",
    "    assert output_shape == expected_output_shape, \"Output shape mismatch\"\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_build_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define Discriminator model\n",
    "def Discriminator(input_nc = 3, ndf=80, n_layers=3):\n",
    "    inputs = tf.keras.layers.Input(shape=(320, 320, input_nc))\n",
    "    x = inputs\n",
    "    nf_mult_prev = 1\n",
    "\n",
    "    # First convolutional layer\n",
    "    x = tf.keras.layers.Conv2D(ndf, kernel_size=4, strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    nf_mult = 1\n",
    "    # Middle convolutional layers\n",
    "    for n in range(1, n_layers):\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n, 5)\n",
    "        x = tf.keras.layers.Conv2D(ndf * nf_mult, kernel_size=4, strides=2, padding='same')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    # Last convolutional layer\n",
    "    nf_mult_prev = nf_mult\n",
    "    nf_mult = min(2 ** n_layers, 5)\n",
    "    x = tf.keras.layers.Conv2D(ndf * nf_mult, kernel_size=4, strides=1, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    # Output convolutional layer\n",
    "    outputs = tf.keras.layers.Conv2D(1, kernel_size=4, strides=1, padding='same')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Test the discriminator model with random input batch\n",
    "def test_build_discriminator():\n",
    "    # Define input shape\n",
    "    input_shape = (320, 320, 3)  # Example input shape\n",
    "\n",
    "    # Build discriminator model\n",
    "    discriminator = Discriminator(3)\n",
    "    \n",
    "    # Generate a random batch of input images\n",
    "    batch_size = 4\n",
    "    random_input_batch = tf.random.uniform((batch_size,) + input_shape, minval=0, maxval=1)\n",
    "    \n",
    "    # Forward pass through the discriminator\n",
    "    discriminator_output = discriminator(random_input_batch)\n",
    "    \n",
    "    # Check output shape\n",
    "    output_shape = discriminator_output.shape\n",
    "    print(output_shape)\n",
    "    print(discriminator.summary())\n",
    "    expected_output_shape = (batch_size, 40, 40, 1)  # Output shape with batch dimension\n",
    "    \n",
    "    # Check if the output shape matches the expected output shape\n",
    "    assert output_shape == expected_output_shape, \"Output shape mismatch\"\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_build_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Adversarial Loss\n",
    "def adversarial_loss(discriminator, generated):\n",
    "    fake_output = discriminator(generated)\n",
    "    return tf.reduce_mean(tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(fake_output), fake_output))\n",
    "\n",
    "\n",
    "# Cycle Consistency Loss\n",
    "def cycle_consistency_loss(real_images, cycled_images, lambda_weight=10):\n",
    "    real_images = tf.cast(real_images, tf.float32)\n",
    "    cycled_images = tf.cast(cycled_images, tf.float32)\n",
    "    loss = tf.reduce_mean(tf.abs(real_images - cycled_images))\n",
    "    return lambda_weight * loss\n",
    "\n",
    "# Identity Loss\n",
    "# def identity_loss(real_images, same_images, lambda_weight=0.5):\n",
    "#     real_images = tf.cast(real_images, tf.float32)\n",
    "#     same_images = tf.cast(same_images, tf.float32)\n",
    "#     loss = tf.reduce_mean(tf.abs(real_images - same_images))\n",
    "#     return lambda_weight * 0.5 * loss\n",
    "\n",
    "\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_batch(generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet, batch_real, batch_monet):\n",
    "    with tf.GradientTape() as gen_R2M_tape, tf.GradientTape() as gen_M2R_tape, \\\n",
    "         tf.GradientTape() as disc_r_tape, tf.GradientTape() as disc_m_tape:\n",
    "\n",
    "        # Generate fake images\n",
    "        fake_monet = generator_real2monet(batch_real, training=True)\n",
    "        fake_real = generator_monet2real(batch_monet, training=True)\n",
    "\n",
    "        # Generate reconstructed images\n",
    "        reconstr_real = generator_monet2real(fake_monet, training=True)\n",
    "        reconstr_monet = generator_real2monet(fake_real, training=True)\n",
    "\n",
    "        #  Identity mapping loss\n",
    "        # identity_loss_real = identity_loss(batch_real, reconstr_real)\n",
    "        # identity_loss_monet = identity_loss(batch_monet, reconstr_monet)\n",
    "\n",
    "        # Adversarial loss\n",
    "        adv_loss_R2M = adversarial_loss(discriminator_monet, fake_monet)\n",
    "        adv_loss_M2R = adversarial_loss(discriminator_real, fake_real)\n",
    "\n",
    "\n",
    "        # Cycle consistency loss\n",
    "        cycle_loss = cycle_consistency_loss(batch_real, reconstr_real) + \\\n",
    "                     cycle_consistency_loss(batch_monet, reconstr_monet)\n",
    "\n",
    "        # Total generator loss\n",
    "        total_gen_R2M_loss = adv_loss_R2M + cycle_loss\n",
    "        total_gen_M2R_loss = adv_loss_M2R + cycle_loss\n",
    "\n",
    "    # Compute gradients of generator loss with respect to generator variables\n",
    "    gen_R2M_gradients = gen_R2M_tape.gradient(total_gen_R2M_loss, generator_real2monet.trainable_variables)\n",
    "    gen_M2R_gradients = gen_M2R_tape.gradient(total_gen_M2R_loss, generator_monet2real.trainable_variables)\n",
    "\n",
    "    # Apply gradients to generator variables\n",
    "    generator_real2monet.optimizer.apply_gradients(zip(gen_R2M_gradients, generator_real2monet.trainable_variables))\n",
    "    generator_monet2real.optimizer.apply_gradients(zip(gen_M2R_gradients, generator_monet2real.trainable_variables))\n",
    "\n",
    "    # Train discriminator real\n",
    "    with tf.GradientTape() as disc_r_tape:\n",
    "        disc_real_real_output = discriminator_real(batch_real, training=True)\n",
    "        disc_real_fake_output = discriminator_real(fake_real, training=True)\n",
    "\n",
    "        # Compute discriminator real loss\n",
    "        disc_real_loss = discriminator_loss(disc_real_real_output, disc_real_fake_output)\n",
    "\n",
    "    # Compute gradients of discriminator real loss with respect to discriminator real variables\n",
    "    disc_real_gradients = disc_r_tape.gradient(disc_real_loss, discriminator_real.trainable_variables)\n",
    "\n",
    "    # Apply gradients to discriminator real variables\n",
    "    discriminator_real.optimizer.apply_gradients(zip(disc_real_gradients, discriminator_real.trainable_variables))\n",
    "\n",
    "    # Train discriminator monet\n",
    "    with tf.GradientTape() as disc_m_tape:\n",
    "        disc_monet_real_output = discriminator_monet(batch_monet, training=True)\n",
    "        disc_monet_fake_output = discriminator_monet(fake_monet, training=True)\n",
    "\n",
    "        # Compute discriminator monet loss\n",
    "        disc_monet_loss = discriminator_loss(disc_monet_real_output, disc_monet_fake_output)\n",
    "\n",
    "    # Compute gradients of discriminator monet loss with respect to discriminator monet variables\n",
    "    disc_monet_gradients = disc_m_tape.gradient(disc_monet_loss, discriminator_monet.trainable_variables)\n",
    "\n",
    "    # Apply gradients to discriminator monet variables\n",
    "    discriminator_monet.optimizer.apply_gradients(zip(disc_monet_gradients, discriminator_monet.trainable_variables))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_one_batch(generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet, batch_real, batch_monet):\n",
    "    # Forward pass through the network to generate fake and reconstructed images\n",
    "    fake_monet = generator_real2monet(batch_real, training=False)\n",
    "    fake_real = generator_monet2real(batch_monet, training=False)\n",
    "    reconstr_real = generator_monet2real(fake_monet, training=False)\n",
    "    reconstr_monet = generator_real2monet(fake_real, training=False)\n",
    "    \n",
    "    # Compute adversarial losses for both generators\n",
    "    adv_loss_R2M = adversarial_loss(discriminator_monet, fake_monet)\n",
    "    adv_loss_M2R = adversarial_loss(discriminator_real, fake_real)\n",
    "\n",
    "    # Compute cycle consistency loss\n",
    "    cycle_loss = cycle_consistency_loss(batch_real, reconstr_real) + \\\n",
    "                 cycle_consistency_loss(batch_monet, reconstr_monet)\n",
    "    \n",
    "    # Compute total generator loss by combining adversarial and cycle consistency losses\n",
    "    total_gen_R2M_loss = adv_loss_R2M + cycle_loss\n",
    "    total_gen_M2R_loss = adv_loss_M2R + cycle_loss\n",
    "    \n",
    "    # Evaluate discriminator on real and fake images\n",
    "    disc_real_real_output = discriminator_real(batch_real, training=False)\n",
    "    disc_real_fake_output = discriminator_real(fake_real, training=False)\n",
    "    disc_monet_real_output = discriminator_monet(batch_monet, training=False)\n",
    "    disc_monet_fake_output = discriminator_monet(fake_monet, training=False)\n",
    "    \n",
    "    # Compute discriminator losses\n",
    "    disc_real_loss = discriminator_loss(disc_real_real_output, disc_real_fake_output)\n",
    "    disc_monet_loss = discriminator_loss(disc_monet_real_output, disc_monet_fake_output)\n",
    "    \n",
    "    # Return the computed losses without applying gradients or updating the model\n",
    "    return {\n",
    "        \"g_real2monet_loss\": total_gen_R2M_loss,\n",
    "        \"g_monet2real_loss\": total_gen_M2R_loss,\n",
    "        \"d_real_loss\": disc_real_loss,\n",
    "        \"d_monet_loss\": disc_monet_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_one_epoch(generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet, train_dataset, val_dataset):\n",
    "    train_loss_accumulator = {\"g_real2monet_loss\": tf.keras.metrics.Mean(),\n",
    "                              \"g_monet2real_loss\": tf.keras.metrics.Mean(),\n",
    "                              \"d_real_loss\": tf.keras.metrics.Mean(),\n",
    "                              \"d_monet_loss\": tf.keras.metrics.Mean()}\n",
    "    \n",
    "    val_loss_accumulator = {\"g_real2monet_loss\": tf.keras.metrics.Mean(),\n",
    "                            \"g_monet2real_loss\": tf.keras.metrics.Mean(),\n",
    "                            \"d_real_loss\": tf.keras.metrics.Mean(),\n",
    "                            \"d_monet_loss\": tf.keras.metrics.Mean()}\n",
    "    \n",
    "    for batch_real, batch_monet in train_dataset:\n",
    "        train_losses = evaluate_one_batch(generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet, batch_real, batch_monet )\n",
    "        for key, value in train_losses.items():\n",
    "            train_loss_accumulator[key].update_state(value)\n",
    "    \n",
    "    for batch_real, batch_monet in val_dataset:\n",
    "        val_losses = evaluate_one_batch(generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet, batch_real, batch_monet)\n",
    "        for key, value in val_losses.items():\n",
    "            val_loss_accumulator[key].update_state(value)\n",
    "    \n",
    "    # Compute average losses for both training and validation\n",
    "    train_losses_mean = {key: metric.result().numpy() for key, metric in train_loss_accumulator.items()}\n",
    "    val_losses_mean = {key: metric.result().numpy() for key, metric in val_loss_accumulator.items()}\n",
    "    \n",
    "    return train_losses_mean, val_losses_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "def train(generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet, train_dataset, val_dataset, epochs ,save_whights = False):\n",
    "    losses_df = pd.DataFrame()\n",
    "    if save_whights:\n",
    "        models_folder = \"models\"\n",
    "        os.makedirs(models_folder, exist_ok=True)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training phase\n",
    "        progress_bar = tqdm(train_dataset, total=len(train_dataset), desc=f\"Training Epoch {epoch + 1}/{epochs}\")\n",
    "        for batch_real, batch_monet in progress_bar:\n",
    "            train_one_batch(generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet, batch_real, batch_monet)\n",
    "\n",
    "\n",
    "        # Validation phase\n",
    "        train_losses ,val_losses = evaluate_one_epoch(generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet, train_dataset, val_dataset)\n",
    "\n",
    "        train_losses = {f'{key}_train': value for key, value in train_losses.items()}\n",
    "        val_losses = {f'{key}_val': value for key, value in val_losses.items()}\n",
    "    \n",
    "        all_losses = {**train_losses, **val_losses}\n",
    "\n",
    "        losses_df[f'Epoch {epoch+1}'] = all_losses\n",
    "\n",
    "        #save model weights\n",
    "        if save_whights:\n",
    "            generator_real2monet.save_weights(os.path.join(models_folder, f'generator_real2monet_epoch_{epoch+1}.h5'))\n",
    "            generator_monet2real.save_weights(os.path.join(models_folder, f'generator_monet2real_epoch_{epoch+1}.h5'))\n",
    "            discriminator_real.save_weights(os.path.join(models_folder, f'discriminator_real_epoch_{epoch+1}.h5'))\n",
    "            discriminator_monet.save_weights(os.path.join(models_folder, f'discriminator_monet_epoch_{epoch+1}.h5'))\n",
    "\n",
    "    \n",
    "    return losses_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_models(mac = True , beta_1 = 0.5 , lr = 5e-4):\n",
    "    generator_real2monet = Generator()\n",
    "    generator_monet2real = Generator()\n",
    "\n",
    "    discriminator_real = Discriminator()\n",
    "    discriminator_monet = Discriminator()\n",
    "\n",
    "    if mac:\n",
    "        generator_real2monet.compile(optimizer=legacy.Adam(learning_rate=lr, beta_1=beta_1))\n",
    "        generator_monet2real.compile(optimizer=legacy.Adam(learning_rate=lr, beta_1=beta_1))\n",
    "\n",
    "        discriminator_real.compile(optimizer=legacy.Adam(learning_rate=lr, beta_1=beta_1))\n",
    "        discriminator_monet.compile(optimizer=legacy.Adam(learning_rate=lr, beta_1=beta_1))\n",
    "    else:\n",
    "        generator_real2monet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=beta_1))\n",
    "        generator_monet2real.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=beta_1))\n",
    "\n",
    "        discriminator_real.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=beta_1))\n",
    "        discriminator_monet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=beta_1))\n",
    "\n",
    "    return generator_real2monet , generator_monet2real , discriminator_real , discriminator_monet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to understand how many epochs we need to run\n",
    "epochs_ = 20\n",
    "generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet = create_models()\n",
    "losses_df = train(generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet, train_dataset, val_dataset, epochs=epochs_ , save_whights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = [1e-4, 5e-4, 1e-3]\n",
    "beta_1 = [0.3, 0.5, 0.7]\n",
    "epochs = 2\n",
    "dfs_dict = {}\n",
    "\n",
    "for l in lr:\n",
    "    for b in beta_1:\n",
    "        generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet = create_models(lr =l, beta_1=b)\n",
    "        losses_df = train(generator_real2monet, generator_monet2real, discriminator_real, discriminator_monet, train_dataset, val_dataset, epochs=epochs)\n",
    "        dfs_dict[(l, b)] = losses_df\n",
    "\n",
    "merged_df = pd.concat(dfs_dict.values(), keys=dfs_dict.keys(), axis=0)\n",
    "\n",
    "merged_df.reset_index(inplace=True)\n",
    "\n",
    "merged_df.rename(columns={'level_0': 'lr', 'level_1': 'beta_1' ,'level_2': 'type_loss'}, inplace=True)\n",
    "\n",
    "merged_df.to_csv('results.csv', index=False)\n",
    "\n",
    "print(merged_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GAN_monet_colman)",
   "language": "python",
   "name": "data-gan_monet_colman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
